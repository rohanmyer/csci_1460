{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cto6T9ka7ZMl"
   },
   "source": [
    "# 📝 🌍 CSCI 1460 Machine Translation Assignment\n",
    "In this project you will build a neural machine translation model using a Transformer encoder-decoder architecture. The primary goal of the assignment is to **implement the transformer yourself**! Outside of this class, you would likely use existing library implementations of Transformers, but for this assignment, we want you to get direct experience with the pieces that go into the full architecture.\n",
    "\n",
    "The main learning objectives for this assignment are to:\n",
    "\n",
    "1. Understand and implement a Transformer\n",
    "2. Understand how a greedy decoding algorithm works and implement a beam search algorithm\n",
    "3. Understand and implement the BLEU metric for evaluating machine translation\n",
    "4. Understand the visualization of decoder attention during translation\n",
    "\n",
    "***Make sure you're using a GPU runtime when you are training the model!*** Go to Runtime > Change runtime type to make sure you are on a GPU runtime when training these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBgSRG1p7ZMq"
   },
   "source": [
    "# [DO NOT EDIT] Setup and Data Preprocessing\n",
    "\n",
    "### You only need to **read through** and run this section. There are no coding TODOs for you. Feel free to collapse the cell with the arrow on the top left beside \"Setup\" (after reading through it)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37141,
     "status": "ok",
     "timestamp": 1697427424845,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "XCaEFscw9LwA",
    "outputId": "1d54f58e-4bea-43cc-e969-ab6280cca12d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from en-core-web-sm==3.7.0) (3.7.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (67.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from de-core-news-sm==3.7.0) (3.7.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3460,
     "status": "ok",
     "timestamp": 1697427428303,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "b9CDVm3o7ZMr"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import Callable\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1697427428305,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "VU9gI0Sd7ZMu"
   },
   "outputs": [],
   "source": [
    "def seed_everything(s: int):\n",
    "  \"\"\"\n",
    "  This function allows us to set the seed for all of our random functions\n",
    "  so that we can get reproducible results.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  s : int\n",
    "      seed to seed all random functions with\n",
    "  \"\"\"\n",
    "  random.seed(s)\n",
    "  torch.manual_seed(s)\n",
    "  torch.cuda.manual_seed_all(s)\n",
    "  np.random.seed(s)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17080,
     "status": "ok",
     "timestamp": 1697427445381,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "MWHB-3sD7ZMu",
    "outputId": "c2fa8895-b6f2-4258-b4a4-cb4562f2f6cd"
   },
   "outputs": [],
   "source": [
    "# data loaded from multi30k dataset https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "# download and extract data\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "# get tokenizers for English and German\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6541,
     "status": "ok",
     "timestamp": 1697427451901,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "oGLdP4Dbfn0C"
   },
   "outputs": [],
   "source": [
    "def build_vocab(filepath: str, tokenizer: Callable) -> torchtext.vocab.Vocab:\n",
    "    \"\"\"\n",
    "    Builds a vocab based on a given file of sentence and a given tokenizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        filepath from which to get sentences to generate a vocabulary\n",
    "    tokenizer : Callable\n",
    "        function with which to tokenize the sentences\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    torchtext.vocab.Vocab\n",
    "        a PyTorch vocab object\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f: # each string here is a German or English sentence\n",
    "            counter.update(tokenizer(string_))\n",
    "    return torchtext.vocab.vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "# build vocabularies for both languages based on the training data\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "# sets the default behavior for OOV words to be <unk>\n",
    "de_vocab.set_default_index(0)\n",
    "en_vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4186,
     "status": "ok",
     "timestamp": 1697427456067,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "rA2rAmA8fp3g"
   },
   "outputs": [],
   "source": [
    "def data_process(filepaths: list[str]) -> list[tuple[torch.Tensor, torch.Tensor]]:\n",
    "  \"\"\"\n",
    "  Builds a dataset of translated sentences from a list of filepaths\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  filepaths : list[str]\n",
    "      a list containing the filepath to a German dataset and a filepath to\n",
    "      the corresponding English dataset\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list[tuple[torch.Tensor, torch.Tensor]]\n",
    "      a list of tuples of tensors, where each tuple is a pair of translations\n",
    "      (German, English) in the form of tensors where tokenized words are encoded\n",
    "      as vocabulary indices\n",
    "  \"\"\"\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    # raw_de and raw_en are paired sentences in English and German\n",
    "    de_tensor_ = torch.tensor(de_vocab(de_tokenizer(raw_de)), dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor(en_vocab(en_tokenizer(raw_en)), dtype=torch.long)\n",
    "\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "# clean the data by tokenizing sentences and converting them to tensors\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1697427456067,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "di-c_e7XiUxV"
   },
   "outputs": [],
   "source": [
    "# sets default values for the rest of the notebook\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # remember this for later!\n",
    "BATCH_SIZE = 32\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1697427456068,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "-834e9RH7ZMv"
   },
   "outputs": [],
   "source": [
    "def generate_batch(data_batch: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "  \"\"\"\n",
    "  Used as the collate_fn argument when constructing DataLoaders to collate\n",
    "  lists of samples into batches\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  data_batch : list[tuple[torch.Tensor, torch.Tensor]]\n",
    "      a list of tuples of tensors, where each tuple is a pair of translations\n",
    "      (German, English) in the form of tensors where tokenized words are encoded\n",
    "      as vocabulary indices\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  tuple[torch.Tensor, torch.Tensor]\n",
    "      a tuple of batched German sentences and batched English sentences in the\n",
    "      form of tensors of vocabulary indices\n",
    "  \"\"\"\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "  return de_batch, en_batch\n",
    "\n",
    "# create data loaders for the training, validation, and test data\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=generate_batch)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=False, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jesM-6Xz7ZMv"
   },
   "source": [
    "# Part 1: Building a Transformer\n",
    "In this section, we'll implement our own Transformer model using some PyTorch implementations of attention. Transformers were introduced in [2017 by Vaswani et al.](https://arxiv.org/abs/1706.03762) and consist of an encoder-decoder structure seen below.\n",
    "\n",
    "First, in Part 1a, we'll implement the green encoder box, and then in Part 1b, we'll implement the pink decoder. We'll then link these modules together into a `ManualTransformer` class in Part 2c. Let's open up some boxes!\n",
    "\n",
    "![image](https://drive.google.com/uc?id=1gsA8TgBRN020YwBSdtNUBX8xvnM-ZO93)\n",
    "\n",
    "For more help in understanding transformers on a broader, conceptual level, check out Jay Alammar's [illustrated transformer](https://jalammar.github.io/illustrated-transformer/), which is where the rest of the illustrations in this assignment will be coming from. Reading the paper will also be helpful as it has more details (obviously) about the specific implementation details than the illustrated transformer article does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImWF2ZO-7ZMw"
   },
   "source": [
    "### [DO NOT EDIT] Helper Code for ManualEncoder and ManualDecoder\n",
    "\n",
    "This cell includes some modules and functions. **You should take a look at the code to see how it works, but you shouldn't need to edit any of it.**\n",
    "\n",
    "Modules and Functions:\n",
    "* `SublayerConnection`, a module that takes in a sublayer function (e.g. the forward pass of a feedforward layer or the forward pass of an attention module) and performs routine layer normalization and dropout on that sublayer\n",
    "* `PositionwiseFeedForward`, a module that implements the feedforward network that is reused within the Transformer implementation\n",
    "* `clones(module, N)` which takes in a module and returns a [`nn.ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=nn+modulelist#torch.nn.ModuleList) of N clones of that module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1697427456068,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "Ey2vJnHF7ZMx"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    '''Applies a residual connection followed by a layer norm to any sublayer'''\n",
    "    def __init__(self, size: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Initializes a SublayerConnection module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : int\n",
    "            size of the expected input to the module\n",
    "        dropout : float\n",
    "            dropout value to be used after the sublayer\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sublayer: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Normalizes the input, applies the sublayer,\n",
    "        performs a dropout, and then performs a residual connection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            input Tensor\n",
    "        sublayer : nn.Module\n",
    "            layer that a residual connection is performed over\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the forward pass\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' Implements the two-layer feedforward neural network used in the transformer.'''\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a PositionwiseFeedForward module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_model : int\n",
    "            size of the input into and output out of the feedforward layer\n",
    "        d_ff : int\n",
    "            hidden size of the feedforward layer\n",
    "        dropout : float\n",
    "            dropout value used at end of feedforward layer\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Normalizes the input, applies the sublayer,\n",
    "        performs a dropout, and then performs a residual connection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            input Tensor\n",
    "        sublayer : nn.Module\n",
    "            layer that a residual connection is performed over\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the forward pass\n",
    "        \"\"\"\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "\n",
    "def clones(module: nn.Module, N: int) -> nn.ModuleList:\n",
    "    \"\"\"\n",
    "    Creates a nn.ModuleList of N identical copies of the inputted module\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module : nn.Module\n",
    "        module to clone\n",
    "    N : int\n",
    "        number of layers to clone\n",
    "\n",
    "    Returns\n",
    "    nn.ModuleList\n",
    "        iterable list of the cloned layers\n",
    "    ----------\n",
    "\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtCd2gPf7ZMx"
   },
   "source": [
    "## Part 1a: Building an Encoder Block\n",
    "\n",
    "One green Transformer encoder box consists of *N* identical blocks stacked on top of each other.\n",
    "![image](https://drive.google.com/uc?id=1VTYJOtyVx1L5UORqH29mNSIX1Gz0fgd0)\n",
    "\n",
    "\n",
    "To implement one of these green layers, we will create a `ManualEncoderLayer` class, and then duplicate and stack these all together in our `ManualEncoder` class to create the stack of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF6n3pOW7ZMy"
   },
   "source": [
    "### Implementing an Encoder Layer: `ManualEncoderLayer`\n",
    "Each encoder layer consists of:\n",
    "1. A self-attention sublayer (hint: `torch.nn.MultiheadAttention`)\n",
    "2. A feed-forward sublayer (hint: the class we just gave you, `PositionwiseFeedForward`)\n",
    "\n",
    "![image](https://drive.google.com/uc?id=1_miH3R_hRjDT-_CRFoSr_VhcbDm2o04o)\n",
    "\n",
    "\n",
    "Make sure to create a new `SublayerConnection` module for each sublayer which will cleanly implement residual connections (the dotted lines and 'add' in the diagram) and a LayerNorm module ('normalize' in the diagram). For each sublayer, you will then pass a function that calls either your attention module or your feed-forward module into the `SublayerConnection` forward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1697427456068,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "RgbIQ-7i7ZMy"
   },
   "outputs": [],
   "source": [
    "class ManualEncoderLayer(nn.Module):\n",
    "    def __init__(self, size: int, dropout: float, nhead: int, dim_ff: int):\n",
    "        \"\"\"\n",
    "        Initializes a ManualEncoderLayer module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : int\n",
    "            size of the input into and output out of the feedforward layer\n",
    "        dropout : float\n",
    "            dropout value used for attention, residual connection, and\n",
    "            feedforward layers\n",
    "        nhead : int\n",
    "            number of attention heads used in multi-head attention\n",
    "        dim_ff : int\n",
    "            hidden dimension of the feedforward layer\n",
    "        \"\"\"\n",
    "        super(ManualEncoderLayer, self).__init__()\n",
    "        #TODO: Initialize the necessary pieces of the encoder block\n",
    "        self.size = size\n",
    "        self.attention = nn.MultiheadAttention(size, nhead, dropout=dropout)\n",
    "        self.residual_norm1 = SublayerConnection(size=size, dropout=dropout)\n",
    "        self.ff_layer = PositionwiseFeedForward(d_model=size, d_ff=dim_ff, dropout=dropout)\n",
    "        self.residual_norm2 = SublayerConnection(size=size, dropout=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward encoding pass for one layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            input Tensor\n",
    "        padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the input\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the forward pass\n",
    "        \"\"\"\n",
    "        #TODO: Implement the forward pass\n",
    "        attn_func = lambda x: self.attention(x, x, x, key_padding_mask=padding_mask)[0]\n",
    "        out = self.residual_norm1(x, attn_func)\n",
    "        out = self.residual_norm2(out, self.ff_layer)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUgi-mNf7ZMy"
   },
   "source": [
    "### Implementing the Encoder: `ManualEncoder`\n",
    "In the encoder itself, you want to\n",
    "1. create *N* instances of the given ManualEncoderLayer (hint: check out the `clones` function we gave you above) and name them `layers`\n",
    "2. stack together your *N* layers in the forward pass\n",
    "3. perform layer normalization (`nn.LayerNorm`) one more time at the end. You might have to edit your `ManualEncoderLayer` so you can get the required input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1697427456068,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "csgSFyW37ZMy"
   },
   "outputs": [],
   "source": [
    "class ManualEncoder(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, N: int):\n",
    "        \"\"\"\n",
    "        Initializes a ManualEncoder module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : nn.Module\n",
    "            a ManualEncoderLayer\n",
    "        N : int\n",
    "            number of encoder layers in the encoder\n",
    "        \"\"\"\n",
    "        super(ManualEncoder, self).__init__()\n",
    "        #TODO: Initialize the necessary pieces of the encoder\n",
    "        # (Hint, this mostly consists of making copies of your encoder layers)\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(self.layers[-1].size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Full encoder forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            input Tensor\n",
    "        padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the input\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the forward pass\n",
    "        \"\"\"\n",
    "        #TODO: Implement the forward pass\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "          out = layer(out, padding_mask)\n",
    "        return self.norm(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PgQTQXr7ZMy"
   },
   "source": [
    "## Part 1b: Building a Decoder Block\n",
    "The decoder works the same way, with six identical `ManualDecoderLayer`s stacked on top of each other in our `ManualDecoder` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9KQdC8h7ZMz"
   },
   "source": [
    "### Implementing a Decoder Layer: `ManualDecoderLayer`\n",
    "\n",
    "A single decoder layer consists of:\n",
    "1. A self-attention sublayer\n",
    "2. A cross-attention sublayer (encoder-decoder attention)\n",
    "3. A feed-forward sublayer\n",
    "\n",
    "![image](https://drive.google.com/uc?id=1oU4jlveCwWme58UUwwXs4MEGTnFOtpUA)\n",
    "\n",
    "Again, you will want to use a bunch of `SublayerConnection` modules to handle the residual connections and layer normalizations.\n",
    "\n",
    "**Important:** for the analysis code we give you in Part 3, you will have to save the attention weights of the cross-attention sublayer in the variable `self.attn_weights`. Because of the sublayer setup, this will be a little weird. Do your forward pass normally, but then also separately call your cross-attention module with `need_weights=True` to get out and save the attention weights for that section. (look [here](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) for what `need_weights` actually does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1697427456068,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "W3xkpXQk7ZMz"
   },
   "outputs": [],
   "source": [
    "class ManualDecoderLayer(nn.Module):\n",
    "    def __init__(self, size: int, dropout: float, nhead: int, dim_ff: int):\n",
    "        \"\"\"\n",
    "        Initializes a ManualDecoderLayer module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        size : int\n",
    "            size of the input into and output out of the feedforward layer\n",
    "        dropout : float\n",
    "            dropout value used for attention, residual connection, and\n",
    "            feedforward layers\n",
    "        nhead : int\n",
    "            number of attention heads used in multi-head attention\n",
    "        dim_ff : int\n",
    "            hidden dimension of the feedforward layer\n",
    "        \"\"\"\n",
    "        super(ManualDecoderLayer, self).__init__()\n",
    "        #TODO: Initialize the necessary pieces of the decoder block\n",
    "        self.size = size\n",
    "        self.attn_weights = None\n",
    "\n",
    "        self.attention_1 = nn.MultiheadAttention(size, nhead, dropout=dropout)\n",
    "        self.residual_norm1 = SublayerConnection(size=size, dropout=dropout)\n",
    "\n",
    "        self.attention_2 = nn.MultiheadAttention(size, nhead, dropout=dropout)\n",
    "        self.residual_norm2 = SublayerConnection(size=size, dropout=dropout)\n",
    "\n",
    "        self.ff_layer = PositionwiseFeedForward(d_model=size, d_ff=dim_ff, dropout=dropout)\n",
    "        self.residual_norm3 = SublayerConnection(size=size, dropout=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_encoding: torch.Tensor,\n",
    "                src_padding_mask: torch.Tensor, tgt_mask: torch.Tensor, tgt_padding_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward decoding pass for one layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            input Tensor (model's target sequence)\n",
    "        src_encoding : torch.Tensor\n",
    "            output of the encoder\n",
    "        src_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the \"encoded input\" (src_encoding)\n",
    "        tgt_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of tokens in target to\n",
    "            mask attention to\n",
    "        tgt_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the input (target sequence)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the forward pass\n",
    "        \"\"\"\n",
    "        #TODO: Implement the forward pass\n",
    "        attn_func = lambda x: self.attention_1(x, x, x, attn_mask=tgt_mask, key_padding_mask=tgt_padding_mask)[0]\n",
    "        out = self.residual_norm1(x, attn_func)\n",
    "\n",
    "        attn_func = lambda x: self.attention_2(x, src_encoding, src_encoding, key_padding_mask=src_padding_mask)[0]\n",
    "        _, self.attn_weights = self.attention_2(x, src_encoding, src_encoding, key_padding_mask=src_padding_mask, need_weights=True)\n",
    "        out = self.residual_norm2(out, attn_func)\n",
    "\n",
    "        out = self.residual_norm3(out, self.ff_layer)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OLow-mp7ZMz"
   },
   "source": [
    "### Implementing the Decoder: `ManualDecoder`\n",
    "Now, we can combine as many decoder layers as we want in our decoder class, which should look very similar to our `ManualEncoder` class.\n",
    "\n",
    "1. create *N* instances of the given `ManualDecoderLayer` (hint: check out the `clones` function we gave you above) and name them `layers`\n",
    "2. stack together your *N* layers in the forward pass\n",
    "3. perform layer normalization (`nn.LayerNorm`) one more time at the end. You might have to edit your `ManualDecoderLayer` so you can get the required input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1697427456192,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "jm9tPCxz7ZMz"
   },
   "outputs": [],
   "source": [
    "class ManualDecoder(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, N: int):\n",
    "        \"\"\"\n",
    "        Initializes a ManualDecoder module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : nn.Module\n",
    "            a ManualDecoderLayer\n",
    "        N : int\n",
    "            number of decoder layers in the decoder\n",
    "        \"\"\"\n",
    "        super(ManualDecoder, self).__init__()\n",
    "        #TODO: Initialize the necessary pieces of the decoder\n",
    "        # (Hint, the mostly consists of making copies of your decoder layers)\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(self.layers[-1].size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_encoding: torch.Tensor, src_padding_mask: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor, tgt_padding_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Full decoder forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            input Tensor (model's target sequence)\n",
    "        src_encoding : torch.Tensor\n",
    "            output of the encoder\n",
    "        src_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the \"encoded input\" (src_encoding)\n",
    "        tgt_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of tokens in target to\n",
    "            mask attention to\n",
    "        tgt_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the input (target sequence)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the forward pass\n",
    "        \"\"\"\n",
    "        #TODO: Implement the forward pass\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "          out = layer(out, src_encoding, src_padding_mask, tgt_mask, tgt_padding_mask)\n",
    "        return self.norm(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSFSlHFH7ZMz"
   },
   "source": [
    "## Part 1c: Putting Everything Together\n",
    "\n",
    "Finally, we can implement the last few details. We've already taken care of mostly everything on here - recognize the multi-head attention, layer normalization, and feed-forward networks?\n",
    "\n",
    "![image](https://drive.google.com/uc?id=1Rr9ezcbbW8UAaTSfY-7snHeKrVJHAyoz)\n",
    "\n",
    "There's just a few more things to take care of. In the `ManualTransformer` class, you should put together your encoder and decoder and add token embeddings, positional encodings, and a final linear layer `out` to project from the embeddings to the target vocabulary. You can use these pieces to implement `encode` and `decode`, then call these two in `forward`. We have already defined the `TokenEmbedding` and `PositionalEncoding` classes for you, which you should look over to see how they work. **DO NOT EDIT THEM.** Here's some more info about them:\n",
    "\n",
    "* `TokenEmbedding`, basically a PyTorch [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) module except it also multiplies the embeddings by $\\sqrt{d_{model}}$ as specified in Vaswani et al. 2017\n",
    "* `PositionalEncoding`, a PyTorch module whose forward pass adds positional encodings to the inputted embeddings\n",
    "\n",
    "You'll have to implement `get_tgt_mask` as well. This will get used in `decode` to apply masks to the inputted target data. When masking, we use a value of **0/False to keep the token** and **1/True to mask out the token**. We would like you to have boolean masks, you can either use the `.bool()` function or the kwarg `dtype=torch.bool` to convert int/float tensors to booleans. Also make sure that it is on `device`! (The variable `device` is defined in the setup code that you read through.) If you're having trouble figuring out the output shape, take a look at the expected sizes of arguments to `torch.nn.MultiheadAttention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1697427456192,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "sx0rGgue8d54"
   },
   "outputs": [],
   "source": [
    "## DO NOT EDIT THIS CELL\n",
    "class TokenEmbedding(nn.Module):\n",
    "    ''' Embedding layer with weights defined by a vocabulary and an embedding size'''\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        \"\"\"\n",
    "        Initializes a TokenEmbedding module\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int\n",
    "            size of the vocab that embeddings will be from\n",
    "        emb_size : int\n",
    "            embedding size\n",
    "        \"\"\"\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns the embedding of the tokens multiplied by sqrt(emb_size)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : torch.Tensor\n",
    "            input Tensor in the form of batched tokenized sentences using\n",
    "            vocabulary indices\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the embeddings\n",
    "        \"\"\"\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    ''' Positional encoding layer for the transformer. '''\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        \"\"\"\n",
    "        Initializes a PositionalEncoding module. The math here isn't important\n",
    "        to have a firm grasp of, but the transformer explanation\n",
    "        linked above does explain how this works visually. :)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        emb_size : int\n",
    "            embedding size from TokenEmbedding\n",
    "        dropout : float\n",
    "            dropout to be used after encoding all the inputs\n",
    "        maxlen : int\n",
    "            maximum length of the sentence\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns the sum of the token embedding and the positional embedding.\n",
    "        Again, see the transformer visualization for more information.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        token_embedding : torch.Tensor\n",
    "            input Tensor from a TokenEmbedding layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            a positionally-encoded sentence representation\n",
    "        \"\"\"\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1697427456192,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "ZrGd5YvA7ZM0"
   },
   "outputs": [],
   "source": [
    "class ManualTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int, emb_size: int, nhead: int,\n",
    "                 src_vocab_size: int, tgt_vocab_size: int, dim_ff: int = 512, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a ManualTransformer module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_encoder_layers : int\n",
    "            number of encoder layers in the transformer\n",
    "        num_decoder_layers : int\n",
    "            number of decoder layers in the transformer\n",
    "        emb_size : int\n",
    "            embedding size used for token embedding, positional encoding,\n",
    "            and encoding and decoding\n",
    "        nhead : int\n",
    "            number of heads to use for multi-head attention in the encoder and\n",
    "            decoder\n",
    "        src_vocab_size : int\n",
    "            vocab size of the input sentences of the transformer\n",
    "        tgt_vocab_size : int\n",
    "            vocab size of the target sentences of the transformer\n",
    "        dim_ff : int\n",
    "            hidden dimension of the feedforward layers\n",
    "        dropout : float\n",
    "            dropout to be used after encoding all the inputs\n",
    "        \"\"\"\n",
    "        super(ManualTransformer, self).__init__()\n",
    "\n",
    "        # TODO: Initialize the necessary pieces of the transformer\n",
    "        self.encode_embed = TokenEmbedding(vocab_size=src_vocab_size, emb_size=emb_size)\n",
    "        self.decode_embed = TokenEmbedding(vocab_size=tgt_vocab_size, emb_size=emb_size)\n",
    "        self.encode_pos = PositionalEncoding(emb_size=emb_size, dropout=dropout)\n",
    "        self.decode_pos = PositionalEncoding(emb_size=emb_size, dropout=dropout)\n",
    "\n",
    "        encoder_layer = ManualEncoderLayer(size=emb_size, dropout=dropout, nhead=nhead, dim_ff=dim_ff)\n",
    "        self.encoder = ManualEncoder(layer=encoder_layer, N=num_encoder_layers)\n",
    "\n",
    "        decoder_layer = ManualDecoderLayer(size=emb_size, dropout=dropout, nhead=nhead, dim_ff=dim_ff)\n",
    "        self.decoder = ManualDecoder(layer=decoder_layer, N=num_decoder_layers)\n",
    "\n",
    "        self.linear = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        # self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        ### DO NOT EDIT ###\n",
    "        # Initialize parameters with Glorot / fan_avg.\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def get_tgt_mask(self, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns a mask of the target sequence to be used with attention\n",
    "        during decoding\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tgt : torch.Tensor\n",
    "            model's target sequence\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            boolean mask Tensor displaying locations of tokens in target to\n",
    "            mask attention to\n",
    "        \"\"\"\n",
    "        #TODO: Implement next word mask for the target, which masks out all subsequent words\n",
    "          # *hint* - torch.triu\n",
    "\n",
    "        tgt_len = tgt.size(0)\n",
    "        tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len, device=device), diagonal=1)\n",
    "        tgt_mask = tgt_mask.bool()\n",
    "        return tgt_mask\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Full encoding pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        src : torch.Tensor\n",
    "            model's input sequence (in form of input vocab indices)\n",
    "        src_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the input sequence\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the full encoder pass\n",
    "        \"\"\"\n",
    "        #TODO Implement the encode function\n",
    "        embed = self.encode_embed(src)\n",
    "        encode = self.encode_pos(embed)\n",
    "        return self.encoder(encode, src_padding_mask)\n",
    "\n",
    "    def decode(self, tgt: torch.Tensor, src_encoding:torch.Tensor,\n",
    "               src_padding_mask: torch.Tensor, tgt_padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Full decoding pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tgt : torch.Tensor\n",
    "            model's target sequence\n",
    "        src_encoding : torch.Tensor\n",
    "            output of the encoder\n",
    "        src_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the \"encoded input\" (src_encoding)\n",
    "        tgt_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the input (target sequence)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the forward pass\n",
    "        \"\"\"\n",
    "        #TODO Implement the decode function\n",
    "        embed = self.decode_embed(tgt)\n",
    "        encode = self.decode_pos(embed)\n",
    "        tgt_mask = self.get_tgt_mask(encode)\n",
    "        return self.decoder(encode, src_encoding, src_padding_mask, tgt_mask, tgt_padding_mask)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
    "                src_padding_mask: torch.Tensor, tgt_padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Full forward pass of the transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        src : torch.Tensor\n",
    "            model's input sequence (in form of input vocab indices)\n",
    "        tgt : torch.Tensor\n",
    "            model's target sequence (in form of target vocab indices)\n",
    "        src_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the input sequence\n",
    "        tgt_padding_mask : torch.Tensor\n",
    "            boolean mask Tensor displaying locations of padding tokens in\n",
    "            the target sequence\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            output of the forward pass\n",
    "        \"\"\"\n",
    "        #TODO Implement the forward pass\n",
    "        encoded = self.encode(src, src_padding_mask)\n",
    "        decoded = self.decode(tgt, encoded, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "        output = self.linear(decoded)\n",
    "        return output\n",
    "        # return self.softmax(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOfU5d7t7ZM0"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "Here, we'll write the code to actually train and evaluate our `ManualTransformer`. First, you can sanity check and verify that your model is training properly by training for an epoch and making sure that the results look like this (it's okay if the epoch time is different, just make sure the losses are *about* the same):\n",
    "\n",
    "```\n",
    "Epoch 1, Train loss: 3.999, Val loss: 3.041, Epoch time = 46.511s\n",
    "```\n",
    "Once you're sure the model works, you can train it for 15 epochs and proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1697427456192,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "zGBPTMc-kf23"
   },
   "outputs": [],
   "source": [
    "def padding_mask(idx_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a boolean mask where True indicates that the token is a padding token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx_tensor : torch.Tensor\n",
    "        tensor for which a padding mask should be generated\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    torch.Tensor\n",
    "        boolean mask of padding of input tensor\n",
    "    \"\"\"\n",
    "    return (idx_tensor == PAD_IDX).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1697427456192,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "IlJd-5dQ7ZM0"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model:nn.Module, train_dataloader: DataLoader,\n",
    "                loss_fn: nn.Module, optimizer:torch.optim.Optimizer) -> float:\n",
    "    \"\"\"\n",
    "    Trains the inputted model using the provided data, optimizer, and loss\n",
    "    function for one epoch. Returns the average loss of the epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        model to train\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        optimizer to use with training\n",
    "    train_dataloader : DataLoader\n",
    "        training data\n",
    "    loss_fn : nn.Module\n",
    "        loss function to use with training\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    float\n",
    "        epoch average loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_padding_mask = padding_mask(src)\n",
    "        tgt_padding_mask = padding_mask(tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_dataloader)\n",
    "\n",
    "def val(model: nn.Module, val_dataloader: DataLoader, loss_fn: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    Performs validation on the inputted model using the provided data and loss\n",
    "    function for one epoch. Returns the validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        model to train\n",
    "    val_dataloader : DataLoader\n",
    "        validation data\n",
    "    loss_fn : nn.Module\n",
    "        loss function to use to calculate validation loss\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    float\n",
    "        average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for src, tgt in val_dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_padding_mask = padding_mask(src)\n",
    "        tgt_padding_mask = padding_mask(tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1697427456193,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 240
    },
    "id": "88pdxdz97ZM0"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_dataloader : DataLoader, loss_fn: nn.Module, optimizer: torch.optim.Optimizer, n_epochs:int=10):\n",
    "    \"\"\"\n",
    "    Trains the inputted model using the provided data, optimizer, and loss\n",
    "    function for n epochs. Prints the average loss of the training epoch\n",
    "    and the validation loss after each epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        model to train\n",
    "    train_dataloader : DataLoader\n",
    "        training data\n",
    "    loss_fn : nn.Module\n",
    "        loss function to use with training\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        optimizer to use with training\n",
    "    n_epochs : int\n",
    "        number of epochs to train for\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print(f\"Epoch {epoch},\", end = \" \")\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer)\n",
    "        end_time = timer()\n",
    "        print(f\" Train loss: {train_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\", end = \" \")\n",
    "\n",
    "        val_loss = val(model, val_loader, loss_fn)\n",
    "        print(f\" Val loss: {val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOy8a8SD7ZM1",
    "outputId": "faaddba9-7612-4d78-a54e-56f6b51f0c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,  Train loss: 3.993, Epoch time = 25.691s  Val loss: 3.036\n",
      "Epoch 2,  Train loss: 2.813, Epoch time = 25.192s  Val loss: 2.496\n",
      "Epoch 3,  Train loss: 2.343, Epoch time = 25.176s  Val loss: 2.160\n",
      "Epoch 4,  Train loss: 2.037, Epoch time = 25.218s  Val loss: 1.975\n",
      "Epoch 5,  Train loss: 1.815, Epoch time = 25.220s  Val loss: 1.884\n",
      "Epoch 6,  Train loss: 1.645, Epoch time = 25.207s  Val loss: 1.800\n",
      "Epoch 7,  Train loss: 1.506, Epoch time = 25.240s  Val loss: 1.741\n",
      "Epoch 8,  Train loss: 1.390, Epoch time = 25.222s  Val loss: 1.726\n",
      "Epoch 9,  Train loss: 1.291, Epoch time = 25.245s  Val loss: 1.708\n",
      "Epoch 10,  Train loss: 1.201, Epoch time = 25.250s  Val loss: 1.696\n",
      "Epoch 11,  Train loss: 1.124, Epoch time = 25.251s  Val loss: 1.696\n",
      "Epoch 12,  Train loss: 1.051, Epoch time = 25.223s  Val loss: 1.693\n",
      "Epoch 13,  Train loss: 0.985, Epoch time = 25.234s  Val loss: 1.716\n",
      "Epoch 14,  Train loss: 0.928, Epoch time = 25.254s  Val loss: 1.728\n",
      "Epoch 15,  Train loss: 0.873, Epoch time = 25.255s  Val loss: 1.746\n"
     ]
    }
   ],
   "source": [
    "# initialize the Transformer Model\n",
    "manual_transformer = ManualTransformer(num_encoder_layers=3, num_decoder_layers=3, emb_size=512, nhead=8, src_vocab_size=len(de_vocab), tgt_vocab_size=len(en_vocab))\n",
    "\n",
    "# define our loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(manual_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# train the model, change n_epochs to 15 when you're ready!\n",
    "train(manual_transformer, train_loader, loss_fn, optimizer, n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0ZXfiPs7ZM1"
   },
   "source": [
    "# Part 2: Evaluating our Transformer\n",
    "\n",
    "BLEU is a metric between zero and one that measures the quality of a machine translation against a set of high-quality \"ground truth\" reference translations. It is almost impossible to get a perfect score of 1, even for human translators, but it has been shown that BLEU scores correlate with human evaluations of quality. It's difficult to compare BLEU metrics across corpora, but a score of about 30% is generally considered decent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLsYKgbu7ZM1"
   },
   "source": [
    "## Part 2a: Getting Translations Out of Our Model\n",
    "Before we can actually calculate BLEU though, we need to get our outputs from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiXYYMEN7ZM1"
   },
   "source": [
    "### [DO NOT EDIT] Greedy decoding to get outputs\n",
    "Here we've given you some code to get predictions from the trained model. Here's a description of the functions we've given you:\n",
    "- `tokenlist_to_strlist(tokens, vocab)`, which takes in a list of tokens and outputs a list of the strings that correspond to each token\n",
    "- `tokenlist_to_str(tokens, vocab, show_pad=False)`, which calls the function above and then turns the resulting list of strings into a single sentence. If `show_pad` is True, prints out the \\<pad\\> tokens.\n",
    "- `translate_tokens(model, src_tokens)`, which takes in a Transformer model and tokens for a German source sentence and outputs a list of tokens that are an English translation of the sentence\n",
    "- `translate(model, src_sentence)`, which takes in a Transformer model and a German source sentence as a string, and returns a string English translation.\n",
    "- `greedy_decode(model, src, max_len, start_symbol)` This takes in a model, input source token tensor, int max_len and start_symbol index. This function repeated calls the model to produce generated output from the model until the model produces an EOS token or the `max_len` is reached. The first decoder input id is the `start_symbol`. Importantly, this is a **greedy** decode of the model so we take the highest probability token at each sequence step. This returns the sequence of tokens generated by the model.\n",
    "\n",
    "**TODO**\n",
    "\n",
    "- `beam_search(model, src, max_len, start_symbol, num_beams)` For this method you'll have to implement beam search! This can be a very tricky algorithm to implement so **we recommmend adapting the `greedy_decode` method** so that it now uses beam search instead of a greedy search algorithm. We also recommend using `torch.topk` to find the next tokens of the highest probablity and `heapq.nlargest` to find the current best beams to continue searching down. **If you get stuck, move on and come back to it** - you can just use the `greedy_decode` algorithm instead to look at how your model performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "12aKXWs67ZM1"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "import heapq\n",
    "\n",
    "def greedy_decode(model: nn.Module, src: torch.Tensor, max_len: int,\n",
    "                  start_symbol: int) -> tuple[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Function to generate a translation using a greedy decoding algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        trained model to decode from\n",
    "    src : torch.Tensor\n",
    "        sequence inputted into model\n",
    "    max_len : int\n",
    "        maximum length to generate with decoding\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        optimizer to use with training\n",
    "    start_symbol : int\n",
    "        vocabulary index to the start of sentence token\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tuple[torch.Tensor, float]\n",
    "        a tuple of a tensor containing the sentence found using greedy\n",
    "        decode represented using vocabulary indices, and the probability\n",
    "        associated with that sentence\n",
    "    \"\"\"\n",
    "\n",
    "    src = src.to(device)\n",
    "    src_padding_mask = padding_mask(src)\n",
    "\n",
    "    total_prob = 1.0\n",
    "\n",
    "    src_encoding = model.encode(src, src_padding_mask).to(device)\n",
    "    seq = torch.ones((1, 1), dtype=torch.long, device=device).fill_(start_symbol)\n",
    "    for i in range(max_len-1):\n",
    "        tgt_padding_mask = padding_mask(seq)\n",
    "        out = model.decode(seq, src_encoding, src_padding_mask, tgt_padding_mask).to(device)\n",
    "        out = out.transpose(0, 1).to(device)\n",
    "        probs = softmax(model.linear(out[:, -1]), dim=-1)\n",
    "        prob, next_word = torch.max(probs, dim=1)\n",
    "        total_prob *= prob.item()\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        seq = torch.cat([seq, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return seq.flatten(), total_prob\n",
    "\n",
    "def beam_search(model: nn.Module, src: torch.Tensor, max_len: int, start_symbol: int,\n",
    "                beam_size: int) -> tuple[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Function to generate a translation using a beam search algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        trained model to decode from\n",
    "    src : torch.Tensor\n",
    "        sequence inputted into model\n",
    "    max_len : int\n",
    "        maximum length to generate with decoding\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        optimizer to use with training\n",
    "    start_symbol : int\n",
    "        vocabulary index to the start of sentence token\n",
    "    beam_size : int\n",
    "        size of the beam using during the search\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tuple[torch.Tensor, float]\n",
    "        a tuple of a tensor containing the sentence found using beam\n",
    "        search represented using vocabulary indices, and the probability\n",
    "        associated with that sentence\n",
    "    \"\"\"\n",
    "    # TODO: Implement beam search\n",
    "    src = src.to(device)\n",
    "    src_padding_mask = padding_mask(src)\n",
    "\n",
    "    src_encoding = model.encode(src, src_padding_mask).to(device)\n",
    "\n",
    "    # list of (seq, prob)\n",
    "    beams = [(torch.ones((1, 1), dtype=torch.long, device=device).fill_(start_symbol), 1.0)]\n",
    "\n",
    "    for i in range(max_len-1):\n",
    "        new_beams = []\n",
    "\n",
    "        for seq, prob in beams:\n",
    "            if seq[-1] == EOS_IDX:\n",
    "                new_beams.append((seq, prob))\n",
    "                continue\n",
    "\n",
    "            tgt_padding_mask = padding_mask(seq)\n",
    "            out = model.decode(seq, src_encoding, src_padding_mask, tgt_padding_mask).to(device)\n",
    "            out = out.transpose(0, 1).to(device)\n",
    "            probs = softmax(model.linear(out[:, -1]), dim=-1)\n",
    "            topk_probs, topk_words = torch.topk(probs, k=beam_size)\n",
    "\n",
    "            for j in range(beam_size):\n",
    "                word_prob = topk_probs[0][j].item()\n",
    "                word = topk_words[0][j].item()\n",
    "                new_seq = torch.cat([seq, torch.ones(1, 1).type_as(src.data).fill_(word)], dim=0)\n",
    "                new_prob = prob * word_prob\n",
    "\n",
    "                new_beams.append((new_seq, new_prob))\n",
    "\n",
    "        # Keep the top-k beams\n",
    "        beams = heapq.nlargest(beam_size, new_beams, key=lambda x: x[1])\n",
    "\n",
    "    # Find the beam with the highest probability\n",
    "    best_beam = max(beams, key=lambda x: x[1])\n",
    "\n",
    "    return best_beam[0].flatten(), best_beam[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Z0kWZaE1pXny"
   },
   "outputs": [],
   "source": [
    "def tokenlist_to_strlist(tokens: torch.Tensor, vocab: torchtext.vocab.Vocab) -> list[str]:\n",
    "    \"\"\"\n",
    "    Converts a tensor representation of a list of tokens to a list of strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : torch.Tensor\n",
    "        tensor representation of a list of tokens\n",
    "    vocab : torchtext.vocab.Vocab\n",
    "        vocabulary associated with the given tokens\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list[str]\n",
    "        returns a representation of the tokens in the form of a list of word strings\n",
    "    \"\"\"\n",
    "    return vocab.lookup_tokens(list(tokens.cpu().numpy()))\n",
    "\n",
    "def tokenlist_to_str(tokens: torch.Tensor, vocab: torchtext.vocab.Vocab,\n",
    "                     show_pad: bool=False) -> str:\n",
    "    \"\"\"\n",
    "    Converts a tensor representation of a list of tokens to a string, ignoring start\n",
    "    and end of sentence tokens, and optionally ignoring padding tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : torch.Tensor\n",
    "        tensor representation of a list of tokens\n",
    "    vocab : torchtext.vocab.Vocab\n",
    "        vocabulary associated with the given tokens\n",
    "    show_pad : bool\n",
    "        whether the outputted string should contain pad tokens or not\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "        returns a representation of the tokens in the form of a string\n",
    "    \"\"\"\n",
    "    out = \" \".join(tokenlist_to_strlist(tokens, vocab)).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "    if show_pad:\n",
    "        return out\n",
    "    else:\n",
    "        return out.replace(\"<pad>\", \"\")\n",
    "\n",
    "def translate_tokens(model: nn.Module, src_tokens: torch.Tensor) -> tuple[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Takes a tensor of source tokens and \"translates\" them by inputting them into\n",
    "    the model and using either greedy decode or beam search to decode the output\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        trained model to input tokens into\n",
    "    src_tokens : torch.Tensor\n",
    "        tensor representations of input sentences without start and end of\n",
    "        sentence indices\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tuple[torch.Tensor, float]\n",
    "        a tuple of a tensor containing the sentence found using either greedy\n",
    "        decode or beam search represented using vocabulary indices, and the\n",
    "        probability associated with that sentence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src_tokens = torch.cat([torch.tensor([BOS_IDX]), src_tokens, torch.tensor([EOS_IDX])], dim=0).view(-1, 1)\n",
    "    num_tokens = src_tokens.shape[0]\n",
    "\n",
    "    # TODO: uncomment the below line and comment out the greedy_decode line to run beam_search!\n",
    "\n",
    "    tgt_tokens, prob = beam_search(model, src_tokens, num_tokens + 5, BOS_IDX, 5)\n",
    "    # tgt_tokens, prob = greedy_decode(model, src_tokens, num_tokens + 5, BOS_IDX)\n",
    "    return tgt_tokens, prob\n",
    "\n",
    "def translate(model: nn.Module, src_sentence: str,\n",
    "              output_tokens: bool=False) -> tuple[str, torch.Tensor, torch.Tensor] | tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Takes a sentence string and \"translates\" it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        trained model to input tokens into\n",
    "    src_sentence : str\n",
    "        string representation of input sentence\n",
    "    output_tokens : bool\n",
    "        whether the source and target tokens are also outputted\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tuple[str, torch.Tensor, torch.Tensor]\n",
    "        a string representation of the translated sentence, the tensor\n",
    "        representation of the input, and the tensor representation of the\n",
    "        translated sentence. Used in analyze_sentence below\n",
    "    OR\n",
    "    tuple[str, float]\n",
    "        a string represention of the translated sentence and the\n",
    "        probability associated with that sentence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src_tokens = torch.tensor(de_vocab(de_tokenizer(src_sentence)), dtype=torch.long)\n",
    "    tgt_tokens, prob = translate_tokens(model, src_tokens)\n",
    "    translation = tokenlist_to_str(tgt_tokens, en_vocab)\n",
    "    if output_tokens:\n",
    "      return translation, src_tokens, tgt_tokens\n",
    "    else:\n",
    "      return translation, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ptnwy3l7ZM1"
   },
   "source": [
    "### Take a look at the outputs!\n",
    "In this cell, we encourage you to try some sentences in German to see how well the model does at translating them into English! You can even take some examples from `val_data` yourself to test on. `val_data` is a list of `(de_tokenlist, en_tokenlist)` tuples which you can convert to strings using one of the functions above.\n",
    "\n",
    "Try looking up the paper for the Multi30k dataset online, and see where our language data came from. Does that provide an explanation for why some sentences have better translations than others?\n",
    "\n",
    "You can also change around the number of epochs the model trains for and see how that impacts qualitative performance. Just make sure to change it back after before handing things in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HOW_m8j77ZM2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man on a grass field with a dog \n",
      "  Probability: 0.001377223056251423\n",
      "\n",
      " Three children eating ice cream while a group looks on . \n",
      "  Probability: 0.02292289701050411\n",
      "\n",
      " A woman on the beach taking pictures of the ocean . \n",
      "  Probability: 0.03228031466433587\n",
      "\n",
      " A tractor moving dirt for the graffiti of a wall . \n",
      "  Probability: 0.00011148169361452785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(translation: str, prob: float):\n",
    "    \"\"\"\n",
    "    Prints a translation and its associated probability nicely\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    translation : str\n",
    "        translation string\n",
    "    prob : float\n",
    "        probability associated with translation\n",
    "    \"\"\"\n",
    "    print(translation, f\"Probability: {prob}\")\n",
    "    print()\n",
    "\n",
    "pretty_print(*translate(manual_transformer, \"Ein Mann auf einem Maisfeld mit einem Hund\"))\n",
    "pretty_print(*translate(manual_transformer, \"Drei Kinder essen Eis, während ein Elternteil zuschaut\"))\n",
    "pretty_print(*translate(manual_transformer, \"Eine Frau am Strand fotografiert das Meer\"))\n",
    "pretty_print(*translate(manual_transformer, \"Ein Traktor bewegt Erde für den Bau einer Stützmauer.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-CmxmPG7ZM2"
   },
   "source": [
    "## Part 2b: Evaluating Translations using BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-n87QUg7ZM2"
   },
   "source": [
    "### Implement BLEU\n",
    "The overall formula for BLEU looks like this:\n",
    "$ \\text{BLEU} = \\min(1, \\text{exp}(1 - \\frac{\\text{reference length}}{\\text{output length}}))(\\prod\\limits_{i=1}^{n} precision_{i})^{1/n}$\n",
    "\n",
    "\n",
    "To break it down, you have\n",
    "- a **brevity penality** on the left:\n",
    "$\\min(1, \\text{exp}(1 - \\frac{\\text{reference length}}{\\text{output length}}))$\n",
    "- **n-gram precisions** for all of the sentences against the ground truth translations, which for us will be from unigrams to 4-grams. For $i$ from $1$ to $4$, we calculate $precision_i$, which you can find a formula for below.\n",
    "\n",
    "\n",
    "To calculate $precision_i$, follow the below formula,\n",
    "$precision_i = \\cfrac{\\sum{_{snt\\in candidates}}\\sum{_{igram\\in snt}}\\text{min}(m^i_{snt}, m^i_{ref})}{w_i}$\n",
    "\n",
    "where $w_i$ is the total count of $i$-grams in all of the candidate sentences, $m^i_{snt}$ is the count of a unique $i$-gram in $snt$ and $m^i_{ref}$ is the count of that $i$-gram in the corresponding reference sentence.\n",
    "\n",
    "Read more about how to calculate BLEU [here](https://cloud.google.com/translate/automl/docs/evaluate#bleu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vLcFxAdhA244"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def my_corpus_bleu(list_reference: list, list_candidate: list, n: int=4):\n",
    "    \"\"\"\n",
    "    Calculates the BLEU score given a list of references and a list of candidates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_reference : list[list[int]]\n",
    "      list of reference sentences each in the form of a list of indices\n",
    "    list_candidate : list[list[int]]\n",
    "      list of candidate sentences each in the form of a list of indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "      the BLEU score given the list of references and candidates\n",
    "    \"\"\"\n",
    "    #TODO: Implement the BLEU algorithm\n",
    "    def count_ngrams(sentence, num):\n",
    "      grams = {}\n",
    "      for i in range(len(sentence)-num+1):\n",
    "        n_gram = tuple(sentence[i:i+num])\n",
    "        grams[n_gram] = grams.get(n_gram, 0) + 1\n",
    "      return grams\n",
    "\n",
    "    def calc_precision(i):\n",
    "      w = 0\n",
    "      numerator = 0\n",
    "      for candidate, reference in zip(list_candidate, list_reference):\n",
    "        cand_grams = count_ngrams(candidate, i)\n",
    "        ref_grams = count_ngrams(reference, i)\n",
    "\n",
    "        for gram, count in cand_grams.items():\n",
    "          numerator += min(count, ref_grams.get(gram, 0))\n",
    "          w += count\n",
    "      return numerator / w\n",
    "\n",
    "\n",
    "    precision = 1\n",
    "    reference_lengths = sum([len(ref) for ref in list_reference])\n",
    "    candidate_lengths = sum([len(cand) for cand in list_candidate])\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        precision *= calc_precision(i)\n",
    "\n",
    "    brevity_penalty = min(1, math.exp(1 - reference_lengths / candidate_lengths))\n",
    "    precision = math.pow(precision, 1/n)\n",
    "    return brevity_penalty * precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpnPYAglaZx5"
   },
   "source": [
    "### Check your implementation\n",
    "Here are some unit tests you can use to evaluate your implementation against the NLTK implementation (they should be almost the exact same value, within at least 4 decimal points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hYdVaQYsW1iF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student: 0.27221791225495623\n",
      "NLTK: 0.27221791225495623\n",
      "\n",
      "Student: 0.3387551654364098\n",
      "NLTK: 0.33875516543640977\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "toy_references = [\"The NASA Opportunity rover is battling a massive dust storm on Mars .\".split()]\n",
    "toy_hypothesis = [\"A NASA rover is fighting a massive storm on Mars .\".split()]\n",
    "\n",
    "toy_references2 = [\"The quick brown fox jumped over the lazy dog\".split(),\n",
    "                   \"holy crap today is fring friday\".split(),\n",
    "                   \"jesse we have to cook\". split()]\n",
    "toy_hypothesis2 = [\"The fast maroon fox jumped over the tired old dog\".split(),\n",
    "                   \"oh wow today is fring friday\".split(),\n",
    "                   \"jesse we need to cook\".split()]\n",
    "\n",
    "\n",
    "print(\"Student:\", my_corpus_bleu(toy_references, toy_hypothesis, 4))\n",
    "print(\"NLTK:\", corpus_bleu([[lst] for lst in toy_references], toy_hypothesis)) # labels must be a list of list of words for NLTK BLEU\n",
    "print()\n",
    "print(\"Student:\", my_corpus_bleu(toy_references2, toy_hypothesis2, 4))\n",
    "print(\"NLTK:\", corpus_bleu([[lst] for lst in toy_references2], toy_hypothesis2)) # labels must be a list of list of words for NLTK BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJGMnniU7ZM2"
   },
   "source": [
    "### How does our model stack up?\n",
    "Here we use your BLEU metric to evaluate your model! If your `my_corpus_bleu` implementation is correct, your BLEU score should match the NLTK-implemented score. This might take a couple minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9FfiiehDVsF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student BLEU: 0.3942469742066874\n",
      "NLTK BLEU: 0.3942469742066874\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# remember manual_transformer, val_data, and translate_tokens?\n",
    "predictions = [translate_tokens(manual_transformer, t[0])[0].tolist()[1:-1] for t in val_data]\n",
    "student_labels = [t[1].tolist() for t in val_data] # labels is a list of words for student BLEU\n",
    "nltk_labels = [[t[1].tolist()] for t in val_data] # labels must be a list of list of words for NLTK BLEU\n",
    "\n",
    "print(\"Student BLEU:\", my_corpus_bleu(student_labels, predictions))\n",
    "print(\"NLTK BLEU:\", corpus_bleu(nltk_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBI-puNL7ZM3"
   },
   "source": [
    "# Part 3: Analysis and Conceptual Questions\n",
    "\n",
    "In this section, we've given you a function that takes the cross-attention weights you saved in the `ManualDecoderLayer` forward pass and visualizes them. Call `analyze_sentence` and input a German sentence as a string to get a heatmap of the last layer of the decoder in the cross-attention sublayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LzNuyRA87ZM3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_attention(src_tokens : torch.Tensor, tgt_tokens : torch.Tensor,\n",
    "                   attn_weights : torch.Tensor):\n",
    "    \"\"\"\n",
    "    Plots the attention visualizations for the model's translation of a given\n",
    "    sentence, given the tensor representation of the source and translation\n",
    "    as well as the attention weights\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_tokens : torch.Tensor\n",
    "        tensor representation of the input to the model\n",
    "    tgt_tokens : torch.Tensor\n",
    "        tensor representation of the output of the model\n",
    "    attn_weights : torch.Tensor\n",
    "        weights of the attention between the encoder and decoder\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    attn_weights = attn_weights.cpu()\n",
    "    cax = plt.matshow(attn_weights.detach().numpy(), cmap='bone')\n",
    "\n",
    "    src_labels = ['<bos>'] + de_vocab.lookup_tokens(list(src_tokens.cpu().numpy())) + ['<eos>']\n",
    "    tgt_labels = en_vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))[1:]\n",
    "\n",
    "    plt.xticks(range(len(src_labels)), src_labels, rotation=80)\n",
    "    plt.yticks(range(len(tgt_labels)), tgt_labels)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def analyze_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Plots attention visualizations for the model's translation of a given\n",
    "    sentence\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        the sentence to be analyzed\n",
    "    \"\"\"\n",
    "    translation, src_tokens, tgt_tokens = translate(manual_transformer, sentence, output_tokens=True)\n",
    "    attn_weights = manual_transformer.decoder.layers[-1].attn_weights.squeeze()\n",
    "\n",
    "    plot_attention(src_tokens, tgt_tokens, attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "z0glwiB17ZM3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x577.778 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try out different sentences here!\n",
    "analyze_sentence(\"Ich essen gerne Eis mit meinen Kindern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN4MS8ZB7ZM4"
   },
   "source": [
    "## Conceptual Questions\n",
    "- Translate the sentence \"*Drei Kinder essen Eis, während ein Elternteil zuschaut*\" into English and look at the attention map. In English, this sentence means \"Three kids eat ice cream, while a parent watches.\"\n",
    "- Translate the sentence \"*Ich essen gerne Eis mit meinen Kindern*\" into English and look at the attention map. In English, this sentence means \"I like to eat ice cream with my children.\"\n",
    "\n",
    "1. Compare the quality of the translations of these sentences, using the attention maps as an aid, and try translating other German sentences as well. What is the difference between the attention maps for correct and less correct translations?\n",
    "\n",
    "More correct translations tend to have more \"stark\" attention maps. The model is a lot more sure that a certain German word maps to a certain English word, so the attention weight for that pair is high and the surrounding weights are near 0. Less correct translations have \"fuzzier\" attention maps. There appears to be some noise and uncertainty in the attention weights and many words do not have any significant weight at all. For example, the word \"gerne\" does not have a corresponding English word/phrase with a high attention weight (the closest is \"book\").\n",
    "\n",
    "2. Try looking up the paper for the Multi30k dataset online, and see where our language data came from. Does that provide an explanation for why some sentences have better translations than others? Would this model work very well for translating something like a text conversation?\n",
    "\n",
    "    The Multi30k dataset is an extension of the Flickr30k dataset. The Flickr30k dataset is a collection of about 30 thousand images sourced from online photo sharing websites. Each image has 5 descriptions (written in English by Mechanical Turk workers). The Multi30k dataset is comprised of translations and independent descriptions. One English description for each image was translated into German and additionally, 5 descriptions were written in German for each image.\n",
    "   \n",
    "    This explains why sentences that could reasonably be image descriptions (such as \"Three kids eat ice cream, while a parent watches.\") have much better translations than other sentences that are less likely to be image descriptions (such as \"I like to eat ice cream with my children.\").\n",
    "   \n",
    "    This model would not work well for translating a text conversation because most of these sentences would be about a person's thoughts or activities (ex. \"I'm on my way\", \"Are you free for lunch?\", \"Happy birthday!\"). These are very unlike image captions/descriptions, and so would not have good results from a model trained on the Multi30k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsjSyP277ZM4"
   },
   "outputs": [],
   "source": [
    "print(\"Great translation\")\n",
    "analyze_sentence(\"Drei Kinder essen Eis, während ein Elternteil zuschaut\")\n",
    "\n",
    "print(\"Poor translation\")\n",
    "analyze_sentence(\"Ich essen gerne Eis mit meinen Kindern\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
