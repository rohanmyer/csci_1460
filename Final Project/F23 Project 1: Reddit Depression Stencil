{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1947Ktboqqxtn0VnpYg4vcKUx54syqZER","timestamp":1699296624113}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Reddit Depression Final Project\n","Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n","\n","Read through the paper fully before starting the assignment!"],"metadata":{"id":"9jFvbbC6VtZm"}},{"cell_type":"code","source":["!pip install happiestfuntokenizing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BixMtHVZREB","executionInfo":{"status":"ok","timestamp":1699309024286,"user_tz":300,"elapsed":9795,"user":{"displayName":"Rohan Krishnan","userId":"01871686249201658032"}},"outputId":"3b2af754-c3b9-4602-c005-28cc9e8487bc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: happiestfuntokenizing in /usr/local/lib/python3.10/dist-packages (0.0.7)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"FoBxKQ_OVl-j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699309027154,"user_tz":300,"elapsed":2887,"user":{"displayName":"Rohan Krishnan","userId":"01871686249201658032"}},"outputId":"86edf7c5-1e50-4ee3-849b-e9174ba35186"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import cross_validate, cross_val_score, KFold\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","FILEPATH = 'drive/MyDrive/CSCI 1460/Final Project/'"]},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"rcMOTL7mV9T9"}},{"cell_type":"code","source":["# List of depression subreddits in the paper\n","depression_subreddits = [\"Anger\",\n","    \"anhedonia\", \"DeadBedrooms\",\n","    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n","    \"DecisionMaking\", \"shouldi\",\n","    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n","    \"chronicfatigue\", \"Fatigue\",\n","    \"ForeverAlone\", \"lonely\",\n","    \"cry\", \"grief\", \"sad\", \"Sadness\",\n","    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n","    \"insomnia\", \"sleep\",\n","    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n","    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n","    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n","]\n","\n","symptom_classifier = {\n","    \"Anger\": [\"Anger\"],\n","    \"Anhedonia\": [\"anhedonia\", \"DeadBedrooms\"],\n","    \"Anxiety\": [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"],\n","    \"Concentration deficit\": [\"DecisionMaking\", \"shouldi\"],\n","    \"Disordered eating\": [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"],\n","    \"Fatigue\": [\"chronicfatigue\", \"Fatigue\"],\n","    \"Loneliness\": [\"ForeverAlone\", \"lonely\"],\n","    \"Sadmood\": [\"cry\", \"grief\", \"sad\", \"Sadness\"],\n","    \"Self-loathing\": [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"],\n","    \"Sleep problem\": [\"insomnia\", \"sleep\"],\n","    \"Somatic complaint\": [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"],\n","    \"Suicidal thoughts and attempts\": [\"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\"],\n","    \"Worthlessness\": [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n","}"],"metadata":{"id":"ohOK3wCdWpnA","executionInfo":{"status":"ok","timestamp":1699309027154,"user_tz":300,"elapsed":4,"user":{"displayName":"Rohan Krishnan","userId":"01871686249201658032"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","def load(filename):\n","  \"\"\"Load pickles\"\"\"\n","  with open(FILEPATH + filename, 'rb') as pkl_file:\n","    data = pickle.load(pkl_file)\n","  return pd.DataFrame(data)"],"metadata":{"id":"icnzto8GWFlb","executionInfo":{"status":"ok","timestamp":1699309027427,"user_tz":300,"elapsed":276,"user":{"displayName":"Rohan Krishnan","userId":"01871686249201658032"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from os.path import exists\n","\n","def dataset_generation():\n","  \"\"\"Build control and symptom datasets\"\"\"\n","\n","  if exists(f\"{FILEPATH}symptom.pkl\"):\n","    symptom_df = load(\"symptom.pkl\")\n","  else:\n","    posts_df = load(\"student.pkl\")\n","    posts_df = posts_df.loc[posts_df['subreddit'].isin(depression_subreddits)]\n","    posts_df['text_len'] = posts_df['text'].apply(len)\n","    posts_df = posts_df.loc[posts_df['text_len'] >= 10]\n","    symptom_df = posts_df.reset_index(drop=True)\n","    with open(f\"{FILEPATH}symptom.pkl\", 'wb') as f:\n","      pickle.dump(symptom_df, f)\n","      f.close()\n","\n","  if exists(f\"{FILEPATH}control.pkl\"):\n","    control_df = load(\"control.pkl\")\n","  else:\n","    posts_df = load(\"student.pkl\")\n","    posts_df = posts_df.loc[(posts_df['author'].isin(symptom_df['author'])) & (~posts_df['subreddit'].isin(depression_subreddits))]\n","    posts_df['text_len'] = posts_df['text'].apply(len)\n","    posts_df = posts_df.loc[posts_df['text_len'] >= 10]\n","    min_dates = symptom_df[['author', 'created_utc']].groupby('author').min()\n","    min_dates.rename(columns={'created_utc': 'min_date'}, inplace=True)\n","    min_dates['min_date'] = min_dates['min_date'] - (180*24*60*60)\n","    posts_df = posts_df.merge(min_dates, how=\"inner\", on=\"author\")\n","    control_df = posts_df.loc[posts_df['created_utc'] <= posts_df['min_date']].reset_index(drop=True)\n","    with open(f\"{FILEPATH}control.pkl\", 'wb') as f:\n","      pickle.dump(control_df, f)\n","      f.close()\n","\n","  return symptom_df, control_df"],"metadata":{"id":"Wpw9kJiras4B","executionInfo":{"status":"ok","timestamp":1699309027427,"user_tz":300,"elapsed":3,"user":{"displayName":"Rohan Krishnan","userId":"01871686249201658032"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["symptom_df, control_df = dataset_generation()\n","dataset = pd.concat([symptom_df[['text', 'subreddit']], control_df[['text', 'subreddit']]], ignore_index=True)"],"metadata":{"id":"XEudfqCUlX8e","executionInfo":{"status":"ok","timestamp":1699309028014,"user_tz":300,"elapsed":589,"user":{"displayName":"Rohan Krishnan","userId":"01871686249201658032"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n","\n","def tokenize(corpus):\n","  \"\"\"Tokenize\"\"\"\n","  tokenizer = Tokenizer()\n","  return list(map(lambda sentence: tokenizer.tokenize(sentence), corpus))\n","\n","dataset['tokenized'] = tokenize(dataset['text'])"],"metadata":{"id":"MWGVUju_WxuP","executionInfo":{"status":"ok","timestamp":1699309127753,"user_tz":300,"elapsed":99741,"user":{"displayName":"Rohan Krishnan","userId":"01871686249201658032"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","def stop_words():\n","  \"\"\"Find top 100 words from Reddit dataset to use as stop words\"\"\"\n","  posts_df = load(\"student.pkl\")\n","  tokenizer = Tokenizer()\n","  text = \" \".join(posts_df['text'])\n","  words = tokenizer.tokenize(text)\n","  word_counts = Counter(words)\n","\n","  return word_counts.most_common(100)\n","\n","stopwords = stop_words()"],"metadata":{"id":"Q3j9z7UuW3eG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reddit Topics with LDA\n","\n"," - Don't use MALLET (as the paper does), use some other LDA implementation."],"metadata":{"id":"U4I37U1SXAEZ"}},{"cell_type":"code","source":["import gensim\n","from gensim import corpora\n","from gensim.models import LdaMulticore\n","import multiprocessing\n","\n","def lda_vectorizer(tokenized_docs, num_topics=200):\n","  dictionary = corpora.Dictionary(tokenized_docs)\n","  corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n","\n","  lda_model = LdaMulticore(\n","      corpus,\n","      num_topics=num_topics,\n","      id2word=dictionary,\n","      passes=10,\n","      workers=multiprocessing.cpu_count()\n","      )\n","\n","  vectors = []\n","  for doc_bow in corpus:\n","      topic_vector = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n","      probability_vector = [topic_prob for _, topic_prob in topic_vector]\n","      vectors.append(probability_vector)\n","\n","  return vectors"],"metadata":{"id":"HTqUkx3si-5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["no_stop = []\n","for sentence in dataset['tokenized']:\n","  no_stop.append([word for word in sentence if word not in stopwords])\n","\n","lda_vectors = lda_vectorizer(no_stop)"],"metadata":{"id":"yDfkfd8kjx1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RoBERTa Embeddings"],"metadata":{"id":"E0-97hsVXNkF"}},{"cell_type":"code","source":["# TODO: Your RoBERTa code!"],"metadata":{"id":"blx1SWVMXYDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Main"],"metadata":{"id":"rDWxuF2jXtwi"}},{"cell_type":"code","source":["def main(X, y):\n","  \"\"\"\n","  Here's the basic structure of the main block! It should run\n","  5-fold cross validation with random forest to evaluate your RoBERTa and LDA\n","  performance.\n","  \"\"\"\n","\n","  rf_classifier = RandomForestClassifier()\n","  cv = KFold(n_splits=5, shuffle=True)\n","  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n","\n","  # TODO: Print your training and testing scores!\n","  for fold, (train_score, test_score) in enumerate(zip(results['train_score'], results['test_score'])):\n","    print(f\"Fold {fold + 1}:\")\n","    print(f\"Training Score: {train_score:.4f}\")\n","    print(f\"Testing Score: {test_score:.4f}\")"],"metadata":{"id":"koTBPhcDXujb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["control_idx = dataset.loc[~dataset['subreddit'].isin(depression_subreddits)].index\n","control_lda = [lda_vectors[i] for i in control_idx]\n","control_labels = [0] * len(control_lda)\n","\n","for symptom, subreddits in symptom_classifier.items():\n","  symptom_idx = dataset.loc[dataset['subreddit'].isin(subreddits)].index\n","  symptom_lda = [lda_vectors[i] for i in symptom_idx]\n","  symptom_labels = [1] * len(symptom_lda)\n","\n","  X = control_lda + symptom_lda\n","  y = control_labels + symptom_labels\n","\n","  main(X, y)"],"metadata":{"id":"7L6oAe4KstYS"},"execution_count":null,"outputs":[]}]}