{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jFvbbC6VtZm"
   },
   "source": [
    "# Reddit Depression Final Project\n",
    "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n",
    "\n",
    "Read through the paper fully before starting the assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30786,
     "status": "ok",
     "timestamp": 1700502527855,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "0BixMtHVZREB",
    "outputId": "00dfd5e9-6b50-42ff-9265-b425bb4b8ee1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: happiestfuntokenizing in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (0.0.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /oscar/home/rkrish16/cscienv/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install happiestfuntokenizing\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23550,
     "status": "ok",
     "timestamp": 1700502551401,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "FoBxKQ_OVl-j",
    "outputId": "1a005af1-ebc8-4ce0-9312-4b9c7ff3d5c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/rkrish16/cscienv/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from os.path import exists\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import multiprocessing\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# FILEPATH = 'drive/MyDrive/CSCI 1460/Final Project/'\n",
    "FILEPATH = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcMOTL7mV9T9"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1700502551401,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "ohOK3wCdWpnA"
   },
   "outputs": [],
   "source": [
    "# List of depression subreddits in the paper\n",
    "depression_subreddits = [\"Anger\",\n",
    "    \"anhedonia\", \"DeadBedrooms\",\n",
    "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
    "    \"DecisionMaking\", \"shouldi\",\n",
    "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
    "    \"chronicfatigue\", \"Fatigue\",\n",
    "    \"ForeverAlone\", \"lonely\",\n",
    "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
    "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
    "    \"insomnia\", \"sleep\",\n",
    "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
    "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
    "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
    "]\n",
    "\n",
    "# Dictionary mapping symptoms to subreddits\n",
    "symptom_classifier = {\n",
    "    \"Anger\": [\"Anger\"],\n",
    "    \"Anhedonia\": [\"anhedonia\", \"DeadBedrooms\"],\n",
    "    \"Anxiety\": [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"],\n",
    "    \"Concentration deficit\": [\"DecisionMaking\", \"shouldi\"],\n",
    "    \"Disordered eating\": [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"],\n",
    "    \"Fatigue\": [\"chronicfatigue\", \"Fatigue\"],\n",
    "    \"Loneliness\": [\"ForeverAlone\", \"lonely\"],\n",
    "    \"Sad mood\": [\"cry\", \"grief\", \"sad\", \"Sadness\"],\n",
    "    \"Self-loathing\": [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"],\n",
    "    \"Sleep problem\": [\"insomnia\", \"sleep\"],\n",
    "    \"Somatic complaint\": [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"],\n",
    "    \"Suicidal thoughts and attempts\": [\"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\"],\n",
    "    \"Worthlessness\": [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700502551401,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "icnzto8GWFlb"
   },
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "  \"\"\"Load pickle file as DataFrame\"\"\"\n",
    "  with open(FILEPATH + filename, 'rb') as pkl_file:\n",
    "    data = pickle.load(pkl_file)\n",
    "  return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700502551401,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "Wpw9kJiras4B"
   },
   "outputs": [],
   "source": [
    "def build_symptom(raw_df):\n",
    "  # get all posts in depression subreddits\n",
    "  symptom_df = raw_df.loc[\n",
    "      (raw_df['subreddit'].isin(depression_subreddits))\n",
    "      ]\n",
    "  # # filter out posts under 10 words\n",
    "  # symptom_df['text_len'] = symptom_df['text'].apply(len)\n",
    "  # symptom_df = symptom_df.loc[posts_df['text_len'] >= 10]\n",
    "  return symptom_df.reset_index(drop=True)\n",
    "\n",
    "def build_control(raw_df, symptom_df):\n",
    "  # get posts by authors of symptom dataset in other subreddits\n",
    "  control_df = raw_df.loc[\n",
    "      (raw_df['author'].isin(symptom_df['author'])) &\n",
    "      (~raw_df['subreddit'].isin(depression_subreddits))\n",
    "      ]\n",
    "\n",
    "  # # filter out posts under 10 words\n",
    "  # control_df['text_len'] = control_df['text'].apply(len)\n",
    "  # control_df = control_df.loc[posts_df['text_len'] >= 10]\n",
    "\n",
    "  # only posts at least 180 days before author's first depression post\n",
    "  min_dates = symptom_df[['author', 'created_utc']].groupby('author').min()\n",
    "  min_dates.rename(columns={'created_utc': 'min_date'}, inplace=True)\n",
    "  min_dates['min_date'] = min_dates['min_date'] - (180*24*60*60)\n",
    "  control_df = control_df.merge(min_dates, how=\"inner\", on=\"author\")\n",
    "\n",
    "  return control_df.loc[control_df['created_utc'] <= control_df['min_date']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def dataset_generation():\n",
    "  \"\"\"Build control and symptom datasets\"\"\"\n",
    "  # load raw data and filter out 'deleted' authors\n",
    "  rawdata = load(\"student.pkl\")\n",
    "  rawdata = rawdata.loc[rawdata['author'] != '[deleted]'].reset_index(drop=True)\n",
    "\n",
    "  symptom_df = build_symptom(rawdata)\n",
    "\n",
    "  control_df = build_control(rawdata, symptom_df)\n",
    "\n",
    "  return symptom_df, control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700502551401,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "MWGVUju_WxuP"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def tokenize_sentence(sentence):\n",
    "  \"\"\"tokenize a sentence\"\"\"\n",
    "  return tokenizer.tokenize(sentence)\n",
    "\n",
    "def tokenize_in_parallel(corpus):\n",
    "  \"\"\"tokenize an entire corpus (in parallel)\"\"\"\n",
    "  with Pool(processes=4) as pool:\n",
    "    results = list(tqdm(pool.imap(tokenize_sentence, corpus), total=len(corpus)))\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BgblVX9vj3d"
   },
   "source": [
    "Build symptom & control datasets and tokenize all the documents. Save/load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9884,
     "status": "ok",
     "timestamp": 1700502561280,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "XEudfqCUlX8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 98536/98536 [00:36<00:00, 2689.90it/s]\n"
     ]
    }
   ],
   "source": [
    "if exists(f\"{FILEPATH}dataset.pkl\"):\n",
    "    dataset = load(\"dataset.pkl\")\n",
    "else:\n",
    "  symptom_df, control_df = dataset_generation()\n",
    "  dataset = pd.concat([symptom_df[['text', 'subreddit']], control_df[['text', 'subreddit']]], ignore_index=True)\n",
    "  dataset['tokenized'] = tokenize_in_parallel(dataset['text'])\n",
    "\n",
    "  with open(f\"{FILEPATH}dataset.pkl\", 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1700502561280,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "Q3j9z7UuW3eG"
   },
   "outputs": [],
   "source": [
    "def stop_words():\n",
    "  \"\"\"Find top 100 words from Reddit dataset to use as stop words\"\"\"\n",
    "  reddit_data = load(\"student.pkl\")\n",
    "\n",
    "  # tokenize entire corpus\n",
    "  tokenizer = Tokenizer()\n",
    "  reddit_data['tokenized'] = reddit_data['text'].apply(lambda text: \" \".join([t for t in tokenizer.tokenize(text) if t.isalpha()]))\n",
    "\n",
    "  # Create a document-term matrix using CountVectorizer\n",
    "  vectorizer = CountVectorizer(max_features=100)\n",
    "  dtm = vectorizer.fit_transform(reddit_data['tokenized'])\n",
    "\n",
    "  # Find the most frequent words\n",
    "  word_counts = dtm.sum(axis=0)\n",
    "  word_counts = np.array(word_counts).flatten()\n",
    "  word_indices = word_counts.argsort()[::-1]\n",
    "\n",
    "  # Extract the top 100 words\n",
    "  return [vectorizer.get_feature_names_out()[i] for i in word_indices[:100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTpoqdz9v3bs"
   },
   "source": [
    "Load/build a list of stopwords for the reddit dataset. Stopwords are the top 100 words in the entire reddit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1700502561489,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "cA_AbWgwr3qC"
   },
   "outputs": [],
   "source": [
    "if exists(FILEPATH + \"stopwords.txt\"):\n",
    "  stopwords = []\n",
    "  with open(FILEPATH + \"stopwords.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "      stopwords.append(line.strip())\n",
    "else:\n",
    "  stopwords = stop_words()\n",
    "  with open(FILEPATH + \"stopwords.txt\", 'w') as file:\n",
    "    for string in stopwords:\n",
    "      file.write(string + '\\n')\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4I37U1SXAEZ"
   },
   "source": [
    "## Reddit Topics with LDA\n",
    "\n",
    " - Don't use MALLET (as the paper does), use some other LDA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1700502561489,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "HTqUkx3si-5k"
   },
   "outputs": [],
   "source": [
    "def lda_vectorizer(tokenized_docs, num_topics=200):\n",
    "  \"\"\"\n",
    "  Builds list of 200 dimensional LDA vectors for a corpus\n",
    "  Guaranteed that length of output == length of input\n",
    "  \"\"\"\n",
    "  dictionary = corpora.Dictionary(tokenized_docs)\n",
    "  corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "  print(\"Built Corpus\")\n",
    "\n",
    "  lda_model = LdaMulticore(\n",
    "      corpus,\n",
    "      num_topics=num_topics,\n",
    "      id2word=dictionary,\n",
    "      # passes=10,\n",
    "      # alpha=5,\n",
    "      workers=multiprocessing.cpu_count()\n",
    "      )\n",
    "  print(\"Built LDA Model\")\n",
    "\n",
    "  vectors = []\n",
    "  print(\"Building Vectors...\")\n",
    "  for doc_bow in tqdm(corpus):\n",
    "      if len(doc_bow) > 0:\n",
    "        topic_vector = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "        probability_vector = np.array([topic_prob for _, topic_prob in topic_vector])\n",
    "        vectors.append(probability_vector)\n",
    "      else:\n",
    "        vectors.append(None)\n",
    "\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfBkOsIR9Z3t"
   },
   "source": [
    "Remove stopwords and build LDA vectors. Load/save as appropriate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1794,
     "status": "ok",
     "timestamp": 1700503935692,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "yDfkfd8kjx1a"
   },
   "outputs": [],
   "source": [
    "if exists(FILEPATH + 'lda_vectors.pkl'):\n",
    "  np.load(f\"{FILEPATH}lda_vectors.pkl\", allow_pickle=True)\n",
    "else:\n",
    "  no_stop = []\n",
    "  for sentence in dataset['tokenized']:\n",
    "    no_stop.append([word for word in sentence if word not in stopwords])\n",
    "\n",
    "  lda_vectors = lda_vectorizer(no_stop)\n",
    "\n",
    "  with open(f\"{FILEPATH}lda_vectors.pkl\", 'wb') as f:\n",
    "    pickle.dump(lda_vectors, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0-97hsVXNkF"
   },
   "source": [
    "## RoBERTa Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QCofqXA9hS1"
   },
   "source": [
    "Use GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1700504024787,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "heVDAKmOgG0k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1700503507587,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "blx1SWVMXYDp"
   },
   "outputs": [],
   "source": [
    "def roberta_vectorizer(tokenized_docs):\n",
    "  \"\"\"\n",
    "  Builds list of roBERTa vectors for a corpus\n",
    "  Guaranteed that length of output == length of input\n",
    "  \"\"\"\n",
    "  model_name = \"roberta-base\"\n",
    "  tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "  model = RobertaModel.from_pretrained(model_name)\n",
    "  model.to(device)\n",
    "\n",
    "  vectors = []\n",
    "\n",
    "  for doc in tqdm(tokenized_docs):\n",
    "    text = \" \".join(doc)\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    tokens.to(device)\n",
    "\n",
    "    # Encode the post text with RoBERTa\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**tokens)\n",
    "\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    vectors.append(torch.mean(hidden_states, dim=1).squeeze())\n",
    "\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDi5Mvy59_8Z"
   },
   "source": [
    "Build roBERTa vectors. Load/save as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 957,
     "status": "ok",
     "timestamp": 1700503918886,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "tn2nxHjYbBYi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 33.7MB/s]\n",
      "merges.txt: 100%|█████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 24.6MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 27.9MB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 2.12MB/s]\n",
      "model.safetensors: 100%|███████████████████████████████████████████████████████████████████| 499M/499M [00:04<00:00, 115MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 19%|███████████████▌                                                                  | 18698/98536 [04:26<18:59, 70.09it/s]"
     ]
    }
   ],
   "source": [
    "if exists(FILEPATH + 'roberta_vectors.pkl'):\n",
    "  np.load(f\"{FILEPATH}roberta_vectors.pkl\", allow_pickle=True)\n",
    "else:\n",
    "  roberta_vectors = roberta_vectorizer(dataset['tokenized'])\n",
    "\n",
    "  roberta_vectors = [vector.cpu().numpy() for vector in roberta_vectors]\n",
    "    \n",
    "  with open(f\"{FILEPATH}roberta_vectors.pkl\", 'wb') as f:\n",
    "    pickle.dump(roberta_vectors, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vfDVBaT-Dho"
   },
   "source": [
    "Print lengths of LDA and roBERTa vector lists. Used as sanity check of data preprocessing and vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1114,
     "status": "ok",
     "timestamp": 1700503628233,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "QoPCMZKqozUR",
    "outputId": "5db33f34-7f6c-4ce5-887e-08e82cdf2b2e"
   },
   "outputs": [],
   "source": [
    "for symptom, subreddits in symptom_classifier.items():\n",
    "  symptom_idx = dataset.loc[dataset['subreddit'].isin(subreddits)].index\n",
    "\n",
    "  symptom_lda = [lda_vectors[i] for i in symptom_idx if lda_vectors[i] is not None]\n",
    "  symptom_roberta = [roberta_vectors[i] for i in symptom_idx]\n",
    "\n",
    "  print(symptom)\n",
    "  print(\"LDA\", len(symptom_lda))\n",
    "  print(\"roBERTa\", len(symptom_roberta))\n",
    "  print()\n",
    "\n",
    "control_idx = dataset.loc[~dataset['subreddit'].isin(depression_subreddits)].index\n",
    "\n",
    "control_lda = [lda_vectors[i] for i in control_idx if lda_vectors[i] is not None]\n",
    "control_roberta = [roberta_vectors[i] for i in control_idx]\n",
    "\n",
    "print(\"Control\")\n",
    "print(\"LDA\", len(control_lda))\n",
    "print(\"roBERTa\", len(control_roberta))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDWxuF2jXtwi"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1700503513298,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "koTBPhcDXujb"
   },
   "outputs": [],
   "source": [
    "def main(X, y):\n",
    "  \"\"\"\n",
    "  Here's the basic structure of the main block! It should run\n",
    "  5-fold cross validation with random forest to evaluate your RoBERTa and LDA\n",
    "  performance.\n",
    "  \"\"\"\n",
    "\n",
    "  rf_classifier = RandomForestClassifier()\n",
    "  cv = KFold(n_splits=5, shuffle=True)\n",
    "  results = cross_validate(\n",
    "      rf_classifier,\n",
    "      X=X, y=y, cv=cv,\n",
    "      scoring='roc_auc',\n",
    "      return_train_score=True\n",
    "      )\n",
    "\n",
    "  # Print training and testing scores\n",
    "  train_score = sum(results['train_score']) / 5\n",
    "  test_score = sum(results['test_score']) / 5\n",
    "  print(f\"Training Score: {train_score:.4f}\")\n",
    "  print(f\"Testing Score: {test_score:.4f}\")\n",
    "  return train_score, test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWzxxfyA-sDd"
   },
   "outputs": [],
   "source": [
    "# UNUSED\n",
    "def supplement_symptoms(symptom):\n",
    "  \"\"\"Uses keywords to supplement symptoms with very few posts\"\"\"\n",
    "  additions = False\n",
    "  if symptom == 'Anger':\n",
    "    additions = (\n",
    "        (dataset['subreddit'].isin(depression_subreddits)) &\n",
    "        (dataset['text'].str.contains('anger')))\n",
    "  elif symptom == 'Concentration deficit':\n",
    "    additions = (\n",
    "        (dataset['subreddit'].isin(depression_subreddits)) &\n",
    "         ((dataset['text'].str.contains('concentrate')) |\n",
    "          (dataset['text'].str.contains('focus'))))\n",
    "  elif symptom == 'Fatigue':\n",
    "    additions = (\n",
    "        (dataset['subreddit'].isin(depression_subreddits)) &\n",
    "         ((dataset['text'].str.contains('fatigue')) |\n",
    "          (dataset['text'].str.contains('tired'))))\n",
    "  return additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRUkjrIC-PzA"
   },
   "source": [
    "Train and evaluate Random Forest Classifier for each symptom in the dataset (exclude symptoms with very few posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1700503513702,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "7L6oAe4KstYS"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "control_idx = dataset.loc[~dataset['subreddit'].isin(depression_subreddits)].index\n",
    "\n",
    "control_lda = [lda_vectors[i] for i in control_idx if lda_vectors[i] is not None]\n",
    "control_roberta = [roberta_vectors[i] for i in control_idx]\n",
    "\n",
    "for symptom, subreddits in tqdm(symptom_classifier.items()):\n",
    "  if symptom in ['Fatigue', 'Concentration deficit']:\n",
    "    continue\n",
    "\n",
    "  symptom_idx = dataset.loc[\n",
    "      dataset['subreddit'].isin(subreddits)\n",
    "      #  | supplement_symptoms(symptom)\n",
    "      ].index\n",
    "\n",
    "  symptom_lda = [lda_vectors[i] for i in symptom_idx if lda_vectors[i] is not None]\n",
    "  symptom_roberta = [roberta_vectors[i] for i in symptom_idx]\n",
    "\n",
    "\n",
    "  X_lda = control_lda + symptom_lda\n",
    "  X_roberta = control_roberta + symptom_roberta\n",
    "  y_lda = [0] * len(control_lda) + [1] * len(symptom_lda)\n",
    "  y_roberta = [0] * len(control_roberta) + [1] * len(symptom_roberta)\n",
    "\n",
    "  print(symptom, \"LDA\")\n",
    "  _, lda_score = main(X_lda, y_lda)\n",
    "  print()\n",
    "  print(symptom, \"roBERTa\")\n",
    "  _, roberta_score = main(X_roberta, y_roberta)\n",
    "  print()\n",
    "\n",
    "  results.append({\n",
    "      \"Symptom\": symptom,\n",
    "      \"LDA\": lda_score,\n",
    "      \"roBERTa\": roberta_score\n",
    "  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kT8sXGs-dGZ"
   },
   "source": [
    "Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1700503513702,
     "user": {
      "displayName": "Rohan Krishnan",
      "userId": "01871686249201658032"
     },
     "user_tz": 300
    },
    "id": "jMgHIefAjncB"
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(FILEPATH + \"results.csv\")\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1947Ktboqqxtn0VnpYg4vcKUx54syqZER",
     "timestamp": 1699296624113
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
